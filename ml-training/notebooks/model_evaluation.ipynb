{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d51e89e3",
   "metadata": {},
   "source": [
    "## 1. Setup and Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e0e876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "MODELS_DIR = Path('models')\n",
    "DETECTION_WEIGHTS = MODELS_DIR / 'detector' / 'flower_detector' / 'weights' / 'best.pt'\n",
    "CLASSIFIER_WEIGHTS = MODELS_DIR / 'classifier' / 'flower_classifier' / 'weights' / 'best.pt'\n",
    "\n",
    "# Load models\n",
    "print(\"Loading trained models...\")\n",
    "detector = YOLO(str(DETECTION_WEIGHTS))\n",
    "classifier = YOLO(str(CLASSIFIER_WEIGHTS))\n",
    "print(\"✓ Models loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a71721",
   "metadata": {},
   "source": [
    "## 2. Detection Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3cfcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate detection model\n",
    "print(\"Evaluating Detection Model...\\n\")\n",
    "det_metrics = detector.val(data='data/detection/dataset.yaml')\n",
    "\n",
    "# Extract metrics\n",
    "metrics_dict = {\n",
    "    'mAP@0.5': float(det_metrics.box.map50),\n",
    "    'mAP@0.5-0.95': float(det_metrics.box.map),\n",
    "    'Precision': float(det_metrics.box.mp),\n",
    "    'Recall': float(det_metrics.box.mr),\n",
    "}\n",
    "\n",
    "# Visualize metrics\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "metrics_names = list(metrics_dict.keys())\n",
    "metrics_values = list(metrics_dict.values())\n",
    "\n",
    "colors = ['green' if v > 0.85 else 'orange' if v > 0.75 else 'red' for v in metrics_values]\n",
    "bars = ax.bar(metrics_names, metrics_values, color=colors, alpha=0.7, edgecolor='black')\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, metrics_values):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{val:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.axhline(y=0.85, color='g', linestyle='--', alpha=0.5, label='Target (0.85)')\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Detection Model Performance Metrics', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Detection Model Metrics:\")\n",
    "for metric, value in metrics_dict.items():\n",
    "    status = \"✓\" if value > 0.85 else \"⚠\" if value > 0.75 else \"✗\"\n",
    "    print(f\"  {status} {metric:<20} {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c27ca7",
   "metadata": {},
   "source": [
    "## 3. Classification Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8454209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate classification model\n",
    "print(\"Evaluating Classification Model...\\n\")\n",
    "clf_metrics = classifier.val(data='data/classification')\n",
    "\n",
    "# Extract metrics\n",
    "clf_dict = {\n",
    "    'Top-1 Accuracy': float(clf_metrics.top1),\n",
    "    'Top-5 Accuracy': float(clf_metrics.top5),\n",
    "}\n",
    "\n",
    "# Visualize metrics\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "clf_names = list(clf_dict.keys())\n",
    "clf_values = list(clf_dict.values())\n",
    "\n",
    "colors = ['green' if v > 0.85 else 'orange' if v > 0.75 else 'red' for v in clf_values]\n",
    "bars = ax.bar(clf_names, clf_values, color=colors, alpha=0.7, edgecolor='black')\n",
    "\n",
    "for bar, val in zip(bars, clf_values):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{val:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.axhline(y=0.85, color='g', linestyle='--', alpha=0.5, label='Target (0.85)')\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Classification Model Performance', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Classification Model Metrics:\")\n",
    "for metric, value in clf_dict.items():\n",
    "    status = \"✓\" if value > 0.85 else \"⚠\" if value > 0.75 else \"✗\"\n",
    "    print(f\"  {status} {metric:<20} {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e10682",
   "metadata": {},
   "source": [
    "## 4. Inference Speed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0aa96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Get sample images\n",
    "test_dir = Path('data/detection/images/val')\n",
    "test_images = list(test_dir.glob('*.jpg'))[:10]\n",
    "\n",
    "print(\"Measuring inference speed...\\n\")\n",
    "\n",
    "# Test detection model\n",
    "det_times = []\n",
    "for img_path in test_images:\n",
    "    start = time.time()\n",
    "    results = detector.predict(source=str(img_path), imgsz=640, verbose=False)\n",
    "    det_times.append((time.time() - start) * 1000)  # Convert to ms\n",
    "\n",
    "det_avg = np.mean(det_times)\n",
    "det_std = np.std(det_times)\n",
    "\n",
    "# Test classification\n",
    "clf_times = []\n",
    "for img_path in test_images[:5]:\n",
    "    start = time.time()\n",
    "    results = classifier.predict(source=str(img_path), imgsz=224, verbose=False)\n",
    "    clf_times.append((time.time() - start) * 1000)\n",
    "\n",
    "clf_avg = np.mean(clf_times)\n",
    "clf_std = np.std(clf_times)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Detection timing\n",
    "axes[0].hist(det_times, bins=10, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(det_avg, color='red', linestyle='--', linewidth=2, label=f'Mean: {det_avg:.2f}ms')\n",
    "axes[0].set_xlabel('Inference Time (ms)', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Detection Model Inference Time Distribution', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Classification timing\n",
    "axes[1].hist(clf_times, bins=5, color='lightcoral', edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(clf_avg, color='red', linestyle='--', linewidth=2, label=f'Mean: {clf_avg:.2f}ms')\n",
    "axes[1].set_xlabel('Inference Time (ms)', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].set_title('Classification Model Inference Time', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Detection Model:\")\n",
    "print(f\"  Average: {det_avg:.2f}ms\")\n",
    "print(f\"  Std Dev: {det_std:.2f}ms\")\n",
    "print(f\"  FPS: {1000/det_avg:.1f}\")\n",
    "print(f\"  Target: <33ms (30 FPS) {'✓' if det_avg < 33 else '✗'}\")\n",
    "\n",
    "print(f\"\\nClassification Model:\")\n",
    "print(f\"  Average: {clf_avg:.2f}ms\")\n",
    "print(f\"  Std Dev: {clf_std:.2f}ms\")\n",
    "print(f\"  FPS: {1000/clf_avg:.1f}\")\n",
    "print(f\"  Target: <10ms {'✓' if clf_avg < 10 else '⚠'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a99b1a",
   "metadata": {},
   "source": [
    "## 5. Sample Predictions Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c121f311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "test_images = list(Path('data/detection/images/val').glob('*.jpg'))[:6]\n",
    "\n",
    "for idx, img_path in enumerate(test_images):\n",
    "    results = detector.predict(source=str(img_path), conf=0.5, verbose=False)\n",
    "    \n",
    "    # Get annotated image\n",
    "    result = results[0]\n",
    "    annotated = result.plot()\n",
    "    \n",
    "    axes[idx].imshow(annotated[:, :, ::-1])  # Convert BGR to RGB\n",
    "    axes[idx].set_title(f'{img_path.name}', fontsize=10)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Sample predictions visualized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa81fdce",
   "metadata": {},
   "source": [
    "## 6. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c4df84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate evaluation summary\n",
    "summary = {\n",
    "    'timestamp': pd.Timestamp.now().isoformat(),\n",
    "    'detection_model': {\n",
    "        'metrics': metrics_dict,\n",
    "        'inference_time_ms': float(det_avg),\n",
    "        'fps': float(1000/det_avg),\n",
    "        'status': 'PASS' if metrics_dict['mAP@0.5'] > 0.85 else 'REVIEW'\n",
    "    },\n",
    "    'classification_model': {\n",
    "        'metrics': clf_dict,\n",
    "        'inference_time_ms': float(clf_avg),\n",
    "        'fps': float(1000/clf_avg),\n",
    "        'status': 'PASS' if clf_dict['Top-1 Accuracy'] > 0.85 else 'REVIEW'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "with open('evaluation_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDetection Model: {summary['detection_model']['status']}\")\n",
    "print(f\"  mAP@0.5: {summary['detection_model']['metrics']['mAP@0.5']:.4f}\")\n",
    "print(f\"  Inference: {summary['detection_model']['inference_time_ms']:.2f}ms ({summary['detection_model']['fps']:.1f} FPS)\")\n",
    "\n",
    "print(f\"\\nClassification Model: {summary['classification_model']['status']}\")\n",
    "print(f\"  Top-1 Accuracy: {summary['classification_model']['metrics']['Top-1 Accuracy']:.4f}\")\n",
    "print(f\"  Inference: {summary['classification_model']['inference_time_ms']:.2f}ms ({summary['classification_model']['fps']:.1f} FPS)\")\n",
    "\n",
    "print(f\"\\n✓ Summary saved to: evaluation_summary.json\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
